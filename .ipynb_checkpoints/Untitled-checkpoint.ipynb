{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8eb311f-fe8e-4953-ad66-8b88386c5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032e837e-9217-4a1c-85a4-dcc040f795e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760830e8-82ac-4829-82c6-a302e99f2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aaf427b-3a05-452a-b2b5-3427bba5ae4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "cvtColor(src, code[, dst[, dstCn]]) -> dst\n",
       ".   @brief Converts an image from one color space to another.\n",
       ".   \n",
       ".   The function converts an input image from one color space to another. In case of a transformation\n",
       ".   to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note\n",
       ".   that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the\n",
       ".   bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue\n",
       ".   component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and\n",
       ".   sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.\n",
       ".   \n",
       ".   The conventional ranges for R, G, and B channel values are:\n",
       ".   -   0 to 255 for CV_8U images\n",
       ".   -   0 to 65535 for CV_16U images\n",
       ".   -   0 to 1 for CV_32F images\n",
       ".   \n",
       ".   In case of linear transformations, the range does not matter. But in case of a non-linear\n",
       ".   transformation, an input RGB image should be normalized to the proper value range to get the correct\n",
       ".   results, for example, for RGB \\f$\\rightarrow\\f$ L\\*u\\*v\\* transformation. For example, if you have a\n",
       ".   32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will\n",
       ".   have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor ,\n",
       ".   you need first to scale the image down:\n",
       ".   @code\n",
       ".       img *= 1./255;\n",
       ".       cvtColor(img, img, COLOR_BGR2Luv);\n",
       ".   @endcode\n",
       ".   If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many\n",
       ".   applications, this will not be noticeable but it is recommended to use 32-bit images in applications\n",
       ".   that need the full range of colors or that convert an image before an operation and then convert\n",
       ".   back.\n",
       ".   \n",
       ".   If conversion adds the alpha channel, its value will set to the maximum of corresponding channel\n",
       ".   range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.\n",
       ".   \n",
       ".   @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision\n",
       ".   floating-point.\n",
       ".   @param dst output image of the same size and depth as src.\n",
       ".   @param code color space conversion code (see #ColorConversionCodes).\n",
       ".   @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n",
       ".   channels is derived automatically from src and code.\n",
       ".   \n",
       ".   @see @ref imgproc_color_conversions\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv2.cvtColor??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f600af-484b-40c9-93a6-7ddf0bd6b4aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mp_holistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmp_holistic\u001b[49m\u001b[38;5;241m.\u001b[39mHolistic(min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m holistic:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[0;32m      4\u001b[0m     \n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# Read Feed\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mp_holistic' is not defined"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "    \n",
    "        # Read Feed\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        # Make Detections\n",
    "        image, results = mediapipe_detection(frame, model)\n",
    "        print(results)\n",
    "    \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', frame)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
